{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91ddc180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\91813\\anaconda3\\lib\\site-packages (2.13.0)\n",
      "Requirement already satisfied: keras in c:\\users\\91813\\anaconda3\\lib\\site-packages (2.13.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\91813\\anaconda3\\lib\\site-packages (1.24.3)\n",
      "Requirement already satisfied: pandas in c:\\users\\91813\\anaconda3\\lib\\site-packages (1.4.2)\n",
      "Requirement already satisfied: nltk in c:\\users\\91813\\anaconda3\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: tensorflow-intel==2.13.0 in c:\\users\\91813\\anaconda3\\lib\\site-packages (from tensorflow) (2.13.0)\n",
      "Requirement already satisfied: tensorboard<2.14,>=2.13 in c:\\users\\91813\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (2.13.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\91813\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in c:\\users\\91813\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (4.5.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\91813\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.12.1)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\91813\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\91813\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.4.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\91813\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (4.24.0)\n",
      "Requirement already satisfied: flatbuffers>=23.1.21 in c:\\users\\91813\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (23.5.26)\n",
      "Requirement already satisfied: setuptools in c:\\users\\91813\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (61.2.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\91813\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in c:\\users\\91813\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (2.13.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\91813\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\91813\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (0.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\91813\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (21.3)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\91813\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (16.0.6)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\91813\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\91813\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\91813\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.57.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\91813\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (3.6.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\91813\\anaconda3\\lib\\site-packages (from pandas) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\91813\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: click in c:\\users\\91813\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\91813\\anaconda3\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\91813\\anaconda3\\lib\\site-packages (from nltk) (2022.3.15)\n",
      "Requirement already satisfied: tqdm in c:\\users\\91813\\anaconda3\\lib\\site-packages (from nltk) (4.64.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\91813\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.13.0->tensorflow) (0.37.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\91813\\anaconda3\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2.0.3)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\91813\\anaconda3\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (3.3.4)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\91813\\anaconda3\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2.22.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\91813\\anaconda3\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2.27.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in c:\\users\\91813\\anaconda3\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (1.0.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\91813\\anaconda3\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (0.7.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\91813\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (4.7.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\91813\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (4.2.2)\n",
      "Requirement already satisfied: urllib3<2.0 in c:\\users\\91813\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (1.26.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\91813\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\91813\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\91813\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\91813\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\91813\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2021.10.8)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\91813\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (3.3)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\91813\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (3.2.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\91813\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\91813\\anaconda3\\lib\\site-packages (from packaging->tensorflow-intel==2.13.0->tensorflow) (3.0.4)\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow keras numpy pandas nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ff67616",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Embedding, SpatialDropout1D\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa527612",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\91813\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\91813\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\91813\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44231c9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweets</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Asked #ChatGPT about what it thinks are the pr...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#ChatGPT tornado has already traveled around t...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This is a great explanation of why #EVs are mo...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>‘if you need to write a box-ticking social med...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Just saw an AI tool making my coffee for me. \\...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>767</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>768</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>769</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>770 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Tweets     label\n",
       "0    Asked #ChatGPT about what it thinks are the pr...  Positive\n",
       "1    #ChatGPT tornado has already traveled around t...   Neutral\n",
       "2    This is a great explanation of why #EVs are mo...  Positive\n",
       "3    ‘if you need to write a box-ticking social med...  Positive\n",
       "4    Just saw an AI tool making my coffee for me. \\...  Positive\n",
       "..                                                 ...       ...\n",
       "765                                                NaN       NaN\n",
       "766                                                NaN       NaN\n",
       "767                                                NaN       NaN\n",
       "768                                                NaN       NaN\n",
       "769                                                NaN       NaN\n",
       "\n",
       "[770 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load train dataset\n",
    "# Load train dataset\n",
    "train_df = pd.read_csv('train (1).csv')\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3f60cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop NaN values\n",
    "train_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17da6443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(tweet):\n",
    "    tweet = tweet.lower()\n",
    "    words = word_tokenize(tweet)\n",
    "    words = [word for word in words if word.isalnum()]\n",
    "    words = [word for word in words if word not in stopwords.words('english')]\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "train_df['cleaned_text'] = train_df['Tweets'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46837371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "max_words = 5000\n",
    "max_len = 100\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words, split=' ')\n",
    "tokenizer.fit_on_texts(train_df['cleaned_text'].values)\n",
    "X_train = tokenizer.texts_to_sequences(train_df['cleaned_text'].values)\n",
    "X_train = pad_sequences(X_train, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b002a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding labels\n",
    "label_dict = {'Positive': 0, 'Neutral': 1, 'Negative': 2}\n",
    "train_df['label_encoded'] = train_df['label'].map(label_dict)\n",
    "y_train = to_categorical(train_df['label_encoded'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe0c46d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>@CoffeeVectors @jenny____r confirmed #ChatGPT has dad jokes https://t.co/tC2Sks2f76</th>\n",
       "      <th>Positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sooooo depressing (our competition as journos ...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I just had a 4 hour deep conversation with #Ch...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I began to explore #chatgpt today.\\n\\nI asked ...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/2023\\n1. Ask #ChatGPT\\n2. Add some content\\n3...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#Explained: Are you soon going to be replaced?...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>Earth's #OpenAIChatGPT  has become sentient an...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>The first response was elaborate with some obv...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>The absolutely terrifying inevitability of #Ch...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>I asked ChatGPT what video would give the most...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>Not every postgrad student trying this prompt ...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>170 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    @CoffeeVectors @jenny____r confirmed #ChatGPT has dad jokes https://t.co/tC2Sks2f76  \\\n",
       "0    Sooooo depressing (our competition as journos ...                                    \n",
       "1    I just had a 4 hour deep conversation with #Ch...                                    \n",
       "2    I began to explore #chatgpt today.\\n\\nI asked ...                                    \n",
       "3    /2023\\n1. Ask #ChatGPT\\n2. Add some content\\n3...                                    \n",
       "4    #Explained: Are you soon going to be replaced?...                                    \n",
       "..                                                 ...                                    \n",
       "165  Earth's #OpenAIChatGPT  has become sentient an...                                    \n",
       "166  The first response was elaborate with some obv...                                    \n",
       "167  The absolutely terrifying inevitability of #Ch...                                    \n",
       "168  I asked ChatGPT what video would give the most...                                    \n",
       "169  Not every postgrad student trying this prompt ...                                    \n",
       "\n",
       "     Positive  \n",
       "0    Negative  \n",
       "1    Positive  \n",
       "2    Negative  \n",
       "3    Positive  \n",
       "4    Negative  \n",
       "..        ...  \n",
       "165  Positive  \n",
       "166  Positive  \n",
       "167  Negative  \n",
       "168   Neutral  \n",
       "169   Neutral  \n",
       "\n",
       "[170 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load test dataset\n",
    "test_df = pd.read_csv('test.csv')\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5de79ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop NaN values\n",
    "test_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "03e9e99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing for test dataset\n",
    "test_df['cleaned_text'] = test_df['Positive'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a95998f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization for test dataset\n",
    "X_test = tokenizer.texts_to_sequences(test_df['cleaned_text'].values)\n",
    "X_test = pad_sequences(X_test, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eeb5812e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding labels for test dataset\n",
    "test_df['label_encoded'] = test_df['Positive'].map(label_dict)\n",
    "y_test = to_categorical(test_df['label_encoded'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "16d5d04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM model\n",
    "embedding_dim = 128\n",
    "lstm_out = 196\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, embedding_dim, input_length=X_train.shape[1]))\n",
    "model.add(SpatialDropout1D(0.4))\n",
    "model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "79c40eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping\n",
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "33321fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "19/19 - 9s - loss: 0.0563 - accuracy: 0.9850 - val_loss: 0.7071 - val_accuracy: 0.6706 - 9s/epoch - 451ms/step\n",
      "Epoch 2/20\n",
      "19/19 - 8s - loss: 0.0377 - accuracy: 0.9917 - val_loss: 0.6247 - val_accuracy: 0.6706 - 8s/epoch - 442ms/step\n",
      "Epoch 3/20\n",
      "19/19 - 9s - loss: 0.0242 - accuracy: 0.9950 - val_loss: 0.6038 - val_accuracy: 0.6706 - 9s/epoch - 460ms/step\n",
      "Epoch 4/20\n",
      "19/19 - 8s - loss: 0.0215 - accuracy: 0.9950 - val_loss: 0.6020 - val_accuracy: 0.6706 - 8s/epoch - 437ms/step\n",
      "Epoch 5/20\n",
      "19/19 - 8s - loss: 0.0168 - accuracy: 0.9950 - val_loss: 0.6086 - val_accuracy: 0.6706 - 8s/epoch - 434ms/step\n",
      "Epoch 6/20\n",
      "19/19 - 8s - loss: 0.0120 - accuracy: 0.9950 - val_loss: 0.6040 - val_accuracy: 0.6706 - 8s/epoch - 431ms/step\n",
      "Epoch 7/20\n",
      "19/19 - 8s - loss: 0.0196 - accuracy: 0.9950 - val_loss: 0.5979 - val_accuracy: 0.6706 - 8s/epoch - 435ms/step\n",
      "Epoch 8/20\n",
      "19/19 - 8s - loss: 0.0449 - accuracy: 0.9883 - val_loss: 0.6761 - val_accuracy: 0.8882 - 8s/epoch - 427ms/step\n",
      "Epoch 9/20\n",
      "19/19 - 8s - loss: 0.0296 - accuracy: 0.9950 - val_loss: 0.6452 - val_accuracy: 0.8882 - 8s/epoch - 428ms/step\n",
      "Epoch 10/20\n",
      "19/19 - 8s - loss: 0.0134 - accuracy: 0.9967 - val_loss: 0.6005 - val_accuracy: 0.8882 - 8s/epoch - 422ms/step\n"
     ]
    }
   ],
   "source": [
    "# Model training\n",
    "batch_size = 32\n",
    "epochs = 20\n",
    "\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), \n",
    "                    epochs=epochs, batch_size=batch_size, verbose=2, callbacks=[earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3fd8ca5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.6004962921142578\n",
      "Test Accuracy: 0.8882352709770203\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Test Loss:\", score[0])\n",
    "print(\"Test Accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983e7fbb",
   "metadata": {},
   "source": [
    "Hyperparameter tuning using techniques like grid search or random search can also be employed to further increase the test accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
